# train.yml

mode: "train"
project_name: "halvest-contrastive"
group_name: "base"

data:
  subsets: ["ict-1", "ict-2", "ict-3", "ict-4"] # ["base-2", "base-4", "base-6", "base-8"]
  batch_size: 50
  tokenizer_name: "FacebookAI/roberta-base"
  max_length: 512
  map_batch_size: 1000
  load_from_cache_file: true
  shuffle: true # Set to false to get "curriculum learning" on unified data

model:
  base_model_name: "FacebookAI/roberta-base"

train:
  tau: 0.07
  # --- optimizer ---
  lr: 2.0e-5
  weight_decay: 0.01
  # --- checkpointing ---
  every_n_train_steps: 5000
  # --- trainer ---
  gather: true
  device: "gpu"
  num_devices: 3
  strategy: "ddp_find_unused_parameters_true"
  process_group_backend: "gloo"
  max_steps: -1
  max_epochs: 1
  log_every_n_steps: 10
  accumulate_grad_batches: 1
  gradient_clip_val: null
  precision: "16-mixed" # 16-mixed, 32
  overfit_batches: 0.0
  # --- wandb ---
  use_wandb: true
  log_model: false
